{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6AfEVheq3Yf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENSEMBLE TECHNIQUE"
      ],
      "metadata": {
        "id": "3Jn6dJat3dJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "ANS.Ensemble learning in machine learning is a technique where multiple models (often called \"weak learners\") are combined to create a stronger, more accurate model.\n",
        "\n",
        "Key Idea:\n",
        "\n",
        "‚ÄúMany models are better than one.‚Äù\n",
        "\n",
        "A single model may make mistakes, but when we combine the predictions of several models, the errors can cancel out and the overall accuracy improves.\n",
        "\n",
        "Think of it like asking many people for their opinion: one person might be wrong, but if most people are correct, the combined decision is more reliable.\n",
        "\n",
        " Main Concepts in Ensemble Learning:\n",
        "\n",
        "Diversity of Models\n",
        "Different models should learn different aspects of the data (not make the same mistakes).\n",
        "\n",
        "Aggregation of Results\n",
        "\n",
        "For classification: majority voting (most common class is chosen).\n",
        "\n",
        "For regression: averaging predictions.\n",
        "\n",
        "Bias‚ÄìVariance Trade-off\n",
        "Ensemble methods reduce variance (overfitting) and sometimes bias, improving generalization.\n",
        "\n",
        "Types of Ensemble Methods:\n",
        "\n",
        "Bagging (Bootstrap Aggregating) ‚Äì trains models in parallel on random subsets of data (e.g., Random Forest).\n",
        "\n",
        "Boosting ‚Äì trains models sequentially, where each new model focuses on correcting the errors of the previous ones (e.g., AdaBoost, Gradient Boosting, XGBoost).\n",
        "\n",
        "Stacking ‚Äì combines different types of models and uses another model (meta-learner) to make the final prediction."
      ],
      "metadata": {
        "id": "LaV0QUR73lkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "Ans.üîë Bagging vs Boosting\n",
        "Feature\tBagging\tBoosting\n",
        "Full form\tBootstrap Aggregating\tNo full form (Boosting = improving weak learners)\n",
        "How models are trained\tModels are trained independently in parallel on random subsets of data\tModels are trained sequentially, each new model focuses on mistakes of the previous one\n",
        "Data sampling\tUses bootstrapped samples (random sampling with replacement)\tUses all data, but gives higher weights to misclassified samples\n",
        "Goal\tReduce variance (avoid overfitting)\tReduce bias and variance (improves accuracy)\n",
        "Final prediction\tAggregates predictions by majority voting (classification) or averaging (regression)\tCombines models using weighted voting (stronger models get more weight)\n",
        "Overfitting risk\tLess prone to overfitting (good stability)\tMore prone to overfitting if too many learners are added\n",
        "Example algorithms\tRandom Forest\tAdaBoost, Gradient Boosting, XGBoost, LightGBM\n",
        "‚úÖ Simple Analogy:\n",
        "\n",
        "Bagging ‚Üí Like asking many friends separately and then taking the majority opinion.\n",
        "\n",
        "Boosting ‚Üí Like asking one friend at a time, and each new friend focuses on correcting the mistakes of the previous ones."
      ],
      "metadata": {
        "id": "1FRcZGxl6ud6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like random forests?\n",
        "\n",
        "Ans. Bootstrap sampling is a statistical method where we create new datasets by randomly sampling (with replacement) from the original dataset.\n",
        "\n",
        "\"With replacement\" means the same data point can appear multiple times in the new sample.\n",
        "\n",
        "Each bootstrap sample is usually the same size as the original dataset, but not identical (because of repetition).\n",
        "\n",
        "Example:\n",
        "Original dataset = [1, 2, 3, 4]\n",
        "Bootstrap sample (size 4) = [2, 4, 2, 1]\n",
        "\n",
        "üîπ Role of Bootstrap Sampling in Bagging (e.g., Random Forests)\n",
        "\n",
        "Diversity of Models\n",
        "\n",
        "Each model (like a decision tree) is trained on a different bootstrap sample.\n",
        "\n",
        "This ensures the models are not identical and make different errors.\n",
        "\n",
        "Variance Reduction\n",
        "\n",
        "Individual decision trees are high variance (unstable).\n",
        "\n",
        "By averaging predictions from many trees trained on different bootstrap samples, Bagging reduces variance and improves stability.\n",
        "\n",
        "Generalization\n",
        "\n",
        "Models trained on slightly different data subsets can generalize better to unseen data.\n",
        "\n",
        "üîπ In Random Forest specifically:\n",
        "\n",
        "Each tree is trained on a bootstrap sample of the dataset.\n",
        "\n",
        "Additionally, at each split in the tree, a random subset of features is considered (adds even more diversity)."
      ],
      "metadata": {
        "id": "NZeFXGCm7Tua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "Ans. In bootstrap sampling, each tree in a Bagging model (like Random Forest) is trained on a bootstrap sample of the data.\n",
        "\n",
        "Since sampling is done with replacement, on average about 63% of the training data is included in a bootstrap sample.\n",
        "\n",
        "The remaining ~37% of data not chosen for that tree are called Out-of-Bag (OOB) samples.\n",
        "\n",
        "Example:\n",
        "Original dataset = [1, 2, 3, 4, 5, 6]\n",
        "Bootstrap sample for Tree 1 = [2, 4, 4, 6, 1, 2]\n",
        "OOB samples = [3, 5]\n",
        "\n",
        "\n",
        "OOB samples act like a built-in validation set:\n",
        "\n",
        "After training each tree, we test its performance on the OOB samples (the data it never saw).\n",
        "\n",
        "This gives an unbiased estimate of error without needing a separate validation dataset."
      ],
      "metadata": {
        "id": "0EkgyTEc79-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "random forest?\n",
        "Ans. üîπ Feature Importance in a Single Decision Tree\n",
        "\n",
        "How it‚Äôs measured:\n",
        "\n",
        "Importance of a feature is based on how much it reduces impurity (e.g., Gini index or entropy for classification, variance for regression) across all the splits where the feature is used.\n",
        "\n",
        "The more a feature contributes to reducing impurity, the higher its importance.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Importance is often dominated by the top splits (features chosen near the root appear more important).\n",
        "\n",
        "Sensitive to data variations ‚Üí small changes in training data can change the tree structure and therefore the feature importance drastically.\n",
        "\n",
        "May give a biased importance if features have many possible split points (continuous variables often seem more important).\n",
        "\n",
        "üîπ Feature Importance in a Random Forest\n",
        "\n",
        "How it‚Äôs measured:\n",
        "\n",
        "Each tree in the forest computes feature importance (like in a single tree).\n",
        "\n",
        "The random forest then averages (or sums) the importances across all trees.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Much more stable than a single tree ‚Üí since it‚Äôs averaged across many trees.\n",
        "\n",
        "Reduces bias toward dominant features, because:\n",
        "\n",
        "Each tree sees a random bootstrap sample of data.\n",
        "\n",
        "At each split, only a random subset of features is considered.\n",
        "\n",
        "Provides a more robust and reliable estimate of feature importance."
      ],
      "metadata": {
        "id": "gWK1HAye8okc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bbsRlrK-9P0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question 6: Write a Python program to:\n",
        "‚óè Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "‚óè Train a Random Forest Classifier\n",
        "‚óè Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "Ans. # Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# 2. Train Random Forest Classifier\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# 3. Get feature importance scores\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# 4. Create a DataFrame for better visualization\n",
        "feat_importances = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# 5. Sort by importance and print top 5\n",
        "top_features = feat_importances.sort_values(by=\"Importance\", ascending=False).head(5)\n",
        "print(\"Top 5 Important Features:\\n\")\n",
        "print(top_features)\n"
      ],
      "metadata": {
        "id": "aF3l1fYB9ZtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT - Top 5 Important Features:\n",
        "\n",
        "                 Feature  Importance\n",
        "23            worst area    0.139357\n",
        "27  worst concave points    0.132225\n",
        "7    mean concave points    0.107046\n",
        "20          worst radius    0.082848\n",
        "22       worst perimeter    0.080850"
      ],
      "metadata": {
        "id": "wq9h_OPF-EJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question 7: Write a Python program to:\n",
        "‚óè Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "‚óè Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        " Ans. # Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 2. Split dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Train a single Decision Tree\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_pred = dt_model.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# 4. Train a Bagging Classifier with Decision Trees\n",
        "bagging_model = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_model.fit(X_train, y_train)\n",
        "bagging_pred = bagging_model.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
        "print(\"Bagging Classifier Accuracy:\", bagging_accuracy)"
      ],
      "metadata": {
        "id": "gJa5ERLC-T3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT - Decision Tree Accuracy: 1.0\n",
        "\n",
        "Bagging Classifier Accuracy: 1.0\n"
      ],
      "metadata": {
        "id": "VLQ9_vUm-1EQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question 8: Write a Python program to:\n",
        "‚óè Train a Random Forest Classifier\n",
        "‚óè Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "‚óè Print the best parameters and final accuracy\n",
        "\n",
        " Ans. # Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Split dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Define model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 4. Define smaller parameter grid for speed\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],   # number of trees\n",
        "    'max_depth': [5, None]       # limit depth or grow fully\n",
        "}\n",
        "\n",
        "# 5. GridSearchCV (3-fold CV for speed)\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# 7. Evaluate on test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Final Accuracy:\", final_accuracy)"
      ],
      "metadata": {
        "id": "kYPgWsXS_Ehc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT - Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
        "\n",
        "Final Accuracy: 0.9649\n"
      ],
      "metadata": {
        "id": "RX91ju_x_6yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question 9: Write a Python program to:\n",
        "‚óè Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "‚óè Compare their Mean Squared Errors (MSE)\n",
        "\n",
        " Ans.# Import libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# 2. Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Bagging Regressor with Decision Trees\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "bagging_pred = bagging_reg.predict(X_test)\n",
        "bagging_mse = mean_squared_error(y_test, bagging_pred)\n",
        "\n",
        "# 4. Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "# 5. Print results\n",
        "print(\"Bagging Regressor MSE:\", bagging_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)"
      ],
      "metadata": {
        "id": "hcarqSH3ADtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT - Bagging Regressor MSE: 0.2548\n",
        "\n",
        "Random Forest Regressor MSE: 0.2003\n"
      ],
      "metadata": {
        "id": "upPby_L-BUVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to\n",
        "\n",
        "predict loan default. You have access to customer demographic and transaction\n",
        " history data.\n",
        "\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "‚óè Choose between Bagging or Boosting\n",
        "‚óè Handle overfitting\n",
        "‚óè Select base models\n",
        "‚óè Evaluate performance using cross-validation\n",
        "‚óè Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        " Ans. üîπ Step 1: Choose between Bagging or Boosting\n",
        "\n",
        "Bagging (e.g., Random Forest):\n",
        "\n",
        "Good if data has high variance (unstable predictions).\n",
        "\n",
        "Works well when base models (like decision trees) tend to overfit.\n",
        "\n",
        "Prioritizes stability.\n",
        "\n",
        "Boosting (e.g., XGBoost, LightGBM):\n",
        "\n",
        "Good if model suffers from high bias (underfitting).\n",
        "\n",
        "Sequentially improves weak learners by focusing on errors.\n",
        "\n",
        "Typically achieves higher accuracy on complex structured datasets (like financial data).\n",
        "\n",
        " Choice: Start with Boosting (XGBoost/LightGBM) because financial data often has complex non-linear relationships and imbalances. Bagging (Random Forest) can be a strong baseline.\n",
        "\n",
        "üîπ Step 2: Handle Overfitting\n",
        "\n",
        "Regularization (Boosting):\n",
        "\n",
        "Use parameters like max_depth, learning_rate, n_estimators carefully.\n",
        "\n",
        "Smaller trees + lower learning rate ‚Üí better generalization.\n",
        "\n",
        "Cross-validation early stopping:\n",
        "\n",
        "Stop training when validation error stops improving.\n",
        "\n",
        "Feature engineering & selection:\n",
        "\n",
        "Remove noisy or highly correlated features.\n",
        "\n",
        "Bagging / Random Forest:\n",
        "\n",
        "Bootstrap sampling + random feature selection reduces overfitting.\n",
        "\n",
        "üîπ Step 3: Select Base Models\n",
        "\n",
        "Decision Trees ‚Üí most common base learners (simple, interpretable).\n",
        "\n",
        "Could also try:\n",
        "\n",
        "Logistic Regression (for stacking as a meta-learner).\n",
        "\n",
        "Gradient Boosted Trees (LightGBM/XGBoost for boosting).\n",
        "\n",
        "For high-dimensional data, try Random Forest as a bagging model.\n",
        "\n",
        " In practice:\n",
        "\n",
        "Random Forest as a baseline.\n",
        "\n",
        "XGBoost/LightGBM as the main boosting approach.\n",
        "\n",
        "üîπ Step 4: Evaluate Performance using Cross-Validation\n",
        "\n",
        "Use Stratified k-Fold Cross-Validation (to keep class balance, since loan default datasets are usually imbalanced).\n",
        "\n",
        "Metrics:\n",
        "\n",
        "AUC-ROC ‚Üí ability to distinguish defaults vs non-defaults.\n",
        "\n",
        "F1-score / Precision-Recall ‚Üí important for imbalanced data (false negatives costly).\n",
        "\n",
        "Confusion Matrix ‚Üí business interpretation.\n",
        "\n",
        "üîπ Step 5: Justify Ensemble Learning in Real-World Context\n",
        "\n",
        "Financial Risk: Wrong predictions (false negatives) = high losses. Ensemble methods reduce variance and bias, making decisions more reliable.\n",
        "\n",
        "Better Generalization: Ensembles capture complex relationships (transaction patterns, demographics) that single models may miss.\n",
        "\n",
        "Robustness: Bagging reduces overfitting; Boosting improves accuracy by correcting mistakes iteratively.\n",
        "\n",
        "Business Impact: More accurate loan default prediction ‚Üí fewer risky loans approved, lower financial losses, higher trust from stakeholders."
      ],
      "metadata": {
        "id": "rtf5l3FzCEqB"
      }
    }
  ]
}